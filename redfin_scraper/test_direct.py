#!/usr/bin/env python3
"""
ì§ì ‘ Scrapy í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import os
import logging
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings
from redfin_scraper.spiders.rss_feed import RssFeedSpider

def main():
    print("ğŸš€ Scrapy ì§ì ‘ ì‹¤í–‰ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.DEBUG,
        format='%(asctime)s [%(name)s] %(levelname)s: %(message)s'
    )
    
    try:
        # í”„ë¡œì íŠ¸ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        settings = get_project_settings()
        print(f"âœ… ì„¤ì • ë¡œë“œ ì™„ë£Œ: {settings.get('BOT_NAME')}")
        
        # ë¡œê¹… ë ˆë²¨ ì„¤ì •
        settings.set('LOG_LEVEL', 'DEBUG')
        print("âœ… ë¡œê¹… ë ˆë²¨ì„ DEBUGë¡œ ì„¤ì •")
        
        # í”„ë¡œì„¸ìŠ¤ ìƒì„±
        process = CrawlerProcess(settings)
        print("âœ… CrawlerProcess ìƒì„± ì™„ë£Œ")
        
        # ìŠ¤íŒŒì´ë” ì¶”ê°€
        process.crawl(RssFeedSpider)
        print("âœ… RSS í”¼ë“œ ìŠ¤íŒŒì´ë” ì¶”ê°€ ì™„ë£Œ")
        
        # ì‹¤í–‰
        print("ğŸ”„ ìŠ¤íŒŒì´ë” ì‹¤í–‰ ì‹œì‘...")
        process.start()
        print("âœ… ìŠ¤íŒŒì´ë” ì‹¤í–‰ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
