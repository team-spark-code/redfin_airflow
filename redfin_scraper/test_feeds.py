#!/usr/bin/env python3
"""
feeds.yamlì„ ì‚¬ìš©í•˜ì—¬ RSS ë°ì´í„° ìŠ¤í¬ë© í…ŒìŠ¤íŠ¸
"""

import yaml
import feedparser
import json
from datetime import datetime
from pathlib import Path

def load_feeds_config():
    """feeds.yaml íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    feeds_path = Path("feeds/feeds.yaml")
    
    try:
        with open(feeds_path, "r", encoding="utf-8") as f:
            config = yaml.safe_load(f)
            print(f"âœ… feeds.yaml ë¡œë“œ ì™„ë£Œ: {len(config.get('feeds', []))}ê°œ í”¼ë“œ")
            return config
    except FileNotFoundError:
        print(f"âŒ feeds.yaml íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {feeds_path}")
        return None
    except Exception as e:
        print(f"âŒ feeds.yaml íŒŒì‹± ì˜¤ë¥˜: {e}")
        return None

def scrape_rss_feed(name, url):
    """ê°œë³„ RSS í”¼ë“œë¥¼ ìŠ¤í¬ë©í•©ë‹ˆë‹¤."""
    print(f"\nğŸ“¡ ìŠ¤í¬ë© ì¤‘: {name} -> {url}")
    
    try:
        # RSS í”¼ë“œ íŒŒì‹±
        feed = feedparser.parse(url)
        
        if feed.bozo:
            print(f"âš ï¸  í”¼ë“œ íŒŒì‹± ê²½ê³ : {feed.bozo_exception}")
        
        print(f"   ğŸ“Š í”¼ë“œ ì œëª©: {feed.feed.get('title', 'Unknown')}")
        print(f"   ğŸ“ ì—”íŠ¸ë¦¬ ìˆ˜: {len(feed.entries)}")
        
        # ì²« ë²ˆì§¸ ëª‡ ê°œ ì—”íŠ¸ë¦¬ë§Œ ì¶œë ¥
        entries = []
        for i, entry in enumerate(feed.entries[:3]):  # ì²˜ìŒ 3ê°œë§Œ
            entry_data = {
                "title": entry.get("title", ""),
                "link": entry.get("link", ""),
                "published": entry.get("published", ""),
                "summary": entry.get("summary", "")[:100] + "..." if entry.get("summary") else "",
                "author": entry.get("author", ""),
                "tags": [tag.term for tag in entry.get("tags", [])] if hasattr(entry, 'tags') else []
            }
            entries.append(entry_data)
            print(f"   ğŸ“„ {i+1}. {entry_data['title'][:60]}...")
        
        return {
            "feed_name": name,
            "feed_url": url,
            "feed_title": feed.feed.get("title", ""),
            "entry_count": len(feed.entries),
            "sample_entries": entries,
            "scraped_at": datetime.now().isoformat()
        }
        
    except Exception as e:
        print(f"   âŒ ìŠ¤í¬ë© ì‹¤íŒ¨: {e}")
        return {
            "feed_name": name,
            "feed_url": url,
            "error": str(e),
            "scraped_at": datetime.now().isoformat()
        }

def main():
    print("ğŸš€ feeds.yaml RSS ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # feeds.yaml ë¡œë“œ
    config = load_feeds_config()
    if not config:
        return
    
    # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì²˜ìŒ 5ê°œ í”¼ë“œë§Œ ì‚¬ìš©
    test_feeds = config.get("feeds", [])[:5]
    print(f"\nğŸ”¬ í…ŒìŠ¤íŠ¸ìš© {len(test_feeds)}ê°œ í”¼ë“œ ì„ íƒ")
    
    # ê° í”¼ë“œ ìŠ¤í¬ë©
    results = []
    for feed in test_feeds:
        name = feed.get("name", "Unknown")
        url = feed.get("url", "")
        group = feed.get("group", "unknown")
        
        print(f"\nğŸ“‹ í”¼ë“œ ì •ë³´: {name} ({group})")
        result = scrape_rss_feed(name, url)
        results.append(result)
    
    # ê²°ê³¼ ì €ì¥
    output_file = f"rss_test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ ê²°ê³¼ê°€ {output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # ìš”ì•½ ì¶œë ¥
    successful = sum(1 for r in results if "error" not in r)
    total = len(results)
    print(f"\nğŸ“Š ìŠ¤í¬ë˜í•‘ ìš”ì•½: {successful}/{total} ì„±ê³µ")

if __name__ == "__main__":
    main()
